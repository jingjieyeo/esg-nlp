{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing ESG report using Natural Language Processing\n",
    "by Chee-Foong on 12 Mar 2021, updated by Jingjie Yeo on 15 Jul 2021\n",
    "\n",
    "\n",
    "## Summary\n",
    "Environment, Social and Corporate Governance (ESG) refers to the three central factors in measuring the sustainability and societal impact of an investment in a company or business.  These criteria help to better determin the future financial performance of companies (return and risk).\n",
    "\n",
    "This analysis extracts text from a ESG report in PDF format from the internet, performs NLP on these information, summaries the key ESG initiatives with WordClouds, TDIDFs and discovers topics by building a Latent Dirichlet Allocation (LDA) model.\n",
    "\n",
    "To keep this exercise as simple as possible, only one ESG report is being used.  Specifically the [Cabot Corp's 2016 Sustainability report](https://www.cabotcorp.com/-/media/files/reports/responsibility/cabot-corporation-sustainability-report-2016.pdf?la=en&rev=15adb0c9eb01497c8a6e6918fa77f5ff).  \n",
    "\n",
    "Given that ESG is a broad topic.  Different companies focus on different aspects of ESG depending on their business operations and culture.  One can potentially ingest more ESG reports from different companies across all sectors and industries to capture relevant ESG topics.  This to be attempted in another analysis.\n",
    "\n",
    "\n",
    "\n",
    "## Reference\n",
    "\n",
    "1. [A data-driven approach to Environmental, Social and Governance](https://databricks.com/blog/2020/07/10/a-data-driven-approach-to-environmental-social-and-governance.html)\n",
    "2. [Higher ESG ratings are generally positively correlated with valuation and profitability while negatively correlated with volatility.](https://corpgov.law.harvard.edu/2020/01/14/esg-matters/)\n",
    "3. [Topic Modeling with Gensim (Python)](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)\n",
    "4. [Citibank's 2019 ESG report](https://www.citigroup.com/citi/about/esg/download/2019/Global-ESG-Report-2019.pdf?ieNocache=967)\n",
    "5. [Databricks - ESG Reports](https://databricks.com/notebooks/esg_notebooks/01_esg_report.html)\n",
    "5. [Databricks - Data Driven ESG Score](https://databricks.com/notebooks/esg_notebooks/02_esg_scoring.html)\n",
    "6. [Databricks - ESG Market Risk](https://databricks.com/notebooks/esg_notebooks/03_esg_market.html)\n",
    "7. [Topic Modeling and Latent Dirichlet Allocation (LDA) in Python](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24)\n",
    "8. [Evaluate Topic Models: Latent Dirichlet Allocation (LDA)](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0)\n",
    "9. [Topic modeling visualization – How to present the results of LDA models?](https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# pd.options.display.max_columns = 50\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 50)\n",
    "\n",
    "plt.rcParams.update({'figure.figsize':(15,6), 'figure.dpi':60})\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.append('../src') \n",
    "from edge import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "\n",
    "# PDF text extraction\n",
    "from pdfminer.layout import LAParams, LTTextBox\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.converter import TextConverter\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "# Others\n",
    "import requests\n",
    "import io\n",
    "import string\n",
    "import re\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "import io\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Create Folder to store .json data\n",
    "DATA_FOLDER = '../data/'\n",
    "createfolder(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "[nltk_data] Downloading package punkt to /Users/Angela/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Angela/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"needn, wasn't, everything, over, call, doesn, having, around, mustn, per, hereafter, how, ourselves, seem, his, being, hers, you're, or, once, anyway, further, that'll, fill, wherever, shouldn, an, could, throughout, empty, anyone, well, ll, moreover, became, serious, thereafter, twenty, is, doesn't, during, ain, namely, to, find, whereupon, often, mustn't, aren't, even, our, so, seemed, toward, full, ten, against, behind, my, thereupon, part, before, thru, made, becoming, won, this, within, might, nowhere, y, hence, on, every, ltd, eg, did, fifty, see, but, ma, one, since, those, detail, in, nor, amount, are, upon, hasn't, latterly, that, him, until, cant, without, thick, bill, formerly, can, whereafter, down, s, fifteen, either, didn, whither, we, himself, found, wouldn, somewhere, where, neither, however, wasn, noone, aren, etc, fire, alone, i, beyond, it's, weren, top, these, nine, be, it, few, inc, re, next, otherwise, your, just, sincere, beside, wherein, nevertheless, done, sometimes, again, wouldn't, put, and, with, here, them, un, under, as, which, must, forty, couldn't, the, hadn't, yourselves, become, you'll, own, least, below, yourself, everywhere, hasnt, you, they, why, back, into, due, front, three, ours, do, go, any, never, give, anyhow, doing, after, she, all, who, anything, de, via, last, he, by, please, shan, interest, than, else, have, will, whoever, she's, almost, seeming, themselves, move, amongst, latter, didn't, whenever, sixty, shouldn't, her, yours, therefore, everyone, becomes, more, theirs, mightn, between, elsewhere, its, whatever, yet, some, thence, isn't, together, don't, therein, had, no, whom, were, too, now, t, about, myself, less, mightn't, each, does, although, also, beforehand, anywhere, except, out, o, would, five, several, onto, hereby, m, eleven, sometime, others, while, won't, take, system, itself, first, was, don, amoungst, off, nobody, haven, something, third, couldnt, none, perhaps, ever, much, thus, then, ie, been, co, at, hadn, former, us, shan't, other, because, name, herein, herself, should, across, d, not, still, mine, for, needn't, besides, hereupon, most, from, a, me, enough, along, both, already, am, seems, whose, when, whether, eight, only, get, whence, up, someone, through, should've, their, whereby, cannot, mill, mostly, afterwards, above, thin, weren't, whole, hundred, thereby, meanwhile, con, hasn, indeed, nothing, side, of, what, cry, such, may, another, keep, if, two, couldn, somehow, you'd, you've, though, there, haven't, towards, whereas, show, among, rather, isn, very, six, four, has, twelve, describe, bottom, many, same, ve, always\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Details\n",
    "\n",
    "Enter required information about the report.  Report must be in PDF format and downloadable from a URL.\n",
    "\n",
    "1. Company Name\n",
    "2. Company Ticker Symbol\n",
    "3. Year of the Report\n",
    "4. URL of the Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "report_company = 'cabot'\n",
    "report_ticker = 'CBT'\n",
    "report_year = '2016'\n",
    "report_url = 'https://www.cabotcorp.com/-/media/files/reports/responsibility/cabot-corporation-sustainability-report-2016.pdf?la=en&rev=15adb0c9eb01497c8a6e6918fa77f5ff'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Extraction\n",
    "Extract information from the PDF report.  This process may take some time.  Do be patient.\n",
    "\n",
    "*You may skip to the last step of this section if you have previously extracted the contents and stored in a json file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def extract_pdf(file, verbose=False):\n",
    "    \n",
    "    if verbose:\n",
    "        print('Processing {}'.format(file))\n",
    "\n",
    "    try:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        laparams = LAParams()\n",
    "\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, laparams=laparams)\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        password = \"\"\n",
    "        maxpages = 0\n",
    "        pagenos = set()\n",
    "\n",
    "        content = []\n",
    "\n",
    "        for page in PDFPage.get_pages(file,\n",
    "                                      pagenos, \n",
    "                                      maxpages=maxpages,\n",
    "                                      password=password,\n",
    "                                      caching=True,\n",
    "                                      check_extractable=False):\n",
    "\n",
    "            page_interpreter.process_page(page)\n",
    "\n",
    "            content.append(fake_file_handle.getvalue())\n",
    "\n",
    "            fake_file_handle.truncate(0)\n",
    "            fake_file_handle.seek(0)        \n",
    "\n",
    "        text = '##PAGE_BREAK##'.join(content)\n",
    "\n",
    "        # close open handles\n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # close open handles\n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def extract_content(url):\n",
    "    \"\"\"\n",
    "    A simple user define function that, given a url, download PDF text content\n",
    "    Parse PDF and return plain text version\n",
    "    \"\"\"\n",
    "    headers={\"User-Agent\":\"Mozilla/5.0\"}\n",
    "\n",
    "    try:\n",
    "        # retrieve PDF binary stream\n",
    "        r = requests.get(url, allow_redirects=True, headers=headers)\n",
    "        \n",
    "        # access pdf content\n",
    "        text = extract_pdf(io.BytesIO(r.content))\n",
    "\n",
    "        # return concatenated content\n",
    "        return text\n",
    "\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "report_content = extract_content(report_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#Storing all information in a json file\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "report = {'company':report_company, 'year':report_year, 'ticker':report_ticker, \n",
    "          'url':report_url, 'content':report_content}\n",
    "\n",
    "with open(DATA_FOLDER + report_company + report_year + '.json', \"w\") as outfile:  \n",
    "    json.dump(report, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Reloading the json file when required\n",
    "with open(DATA_FOLDER + report_company + report_year + '.json') as inputfile:\n",
    "     report = json.load(inputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "### Extracting content by pages and sentences\n",
    "\n",
    "1. Only properly structured sentences are extracted.  Sentences that start with a Capital Letter and ends with a period.  Sentences that are less than 10 words or more than 50 words are ignored.  \n",
    "2. Pages with less than 500 words are excluded from extraction.  Would like to focus on pages with mostly text content.  This should exclude pages like cover and last page.  It should also exclude pages that are more graphical with short notes and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def remove_non_ascii(text):\n",
    "    printable = set(string.printable)\n",
    "    return ''.join(filter(lambda x: x in printable, text))\n",
    "\n",
    "def not_header(line):\n",
    "    # as we're consolidating broken lines into paragraphs, we want to make sure not to include headers\n",
    "    return not line.isupper()\n",
    "\n",
    "def extract_sentences(nlp, text):\n",
    "    \"\"\"\n",
    "    Extracting ESG statements from raw text by removing junk, URLs, etc.\n",
    "    We group consecutive lines into paragraphs and use spacy to parse sentences.\n",
    "    \"\"\"\n",
    "    MIN_WORDS_PER_PAGE = 500\n",
    "    \n",
    "    pages = text.split('##PAGE_BREAK##')\n",
    "#     print('Number of Pages: {}'.format(len(pages)))\n",
    "\n",
    "    lines = []\n",
    "    for page in pages:\n",
    "        \n",
    "        # remove non ASCII characters\n",
    "        text = remove_non_ascii(page)\n",
    "        \n",
    "        if len(text.split(' ')) < MIN_WORDS_PER_PAGE:\n",
    "#             print('Skipped Page: {}'.format(len(text.split(' '))))\n",
    "            continue\n",
    "        \n",
    "        prev = \"\"\n",
    "        for line in text.split('\\n\\n'):\n",
    "            # aggregate consecutive lines where text may be broken down\n",
    "            # only if next line starts with a space or previous does not end with dot.\n",
    "            if(line.startswith(' ') or not prev.endswith('.')):\n",
    "                prev = prev + ' ' + line\n",
    "            else:\n",
    "                # new paragraph\n",
    "                lines.append(prev)\n",
    "                prev = line\n",
    "\n",
    "        # don't forget left-over paragraph\n",
    "        lines.append(prev)\n",
    "        lines.append('##SAME_PAGE##')\n",
    "        \n",
    "    lines = '  '.join(lines).split('##SAME_PAGE##')\n",
    "    \n",
    "    # clean paragraphs from extra space, unwanted characters, urls, etc.\n",
    "    # best effort clean up, consider a more versatile cleaner\n",
    "    \n",
    "    sentences = []\n",
    "    pages_content = []\n",
    "\n",
    "    for line in lines[:-1]:\n",
    "        # removing header number\n",
    "        line = re.sub(r'^\\s?\\d+(.*)$', r'\\1', line)\n",
    "        # removing trailing spaces\n",
    "        line = line.strip()\n",
    "        # words may be split between lines, ensure we link them back together\n",
    "        line = re.sub(r'\\s?-\\s?', '-', line)\n",
    "        # remove space prior to punctuation\n",
    "        line = re.sub(r'\\s?([,:;\\.])', r'\\1', line)\n",
    "        # ESG contains a lot of figures that are not relevant to grammatical structure\n",
    "        line = re.sub(r'\\d{5,}', r' ', line)\n",
    "        # remove emails\n",
    "        line = re.sub(r'\\S*@\\S*\\s?', '', line)\n",
    "        # remove mentions of URLs\n",
    "        line = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', r' ', line)\n",
    "        # remove multiple spaces\n",
    "        line = re.sub(r'\\s+', ' ', line)\n",
    "        # join next line with space\n",
    "        line = re.sub(r' \\n', ' ', line)\n",
    "        line = re.sub(r'.\\n', '. ', line)\n",
    "        line = re.sub(r'\\x0c', ' ', line)\n",
    "        \n",
    "        pages_content.append(str(line).strip())\n",
    "\n",
    "        # split paragraphs into well defined sentences using spacy\n",
    "        for part in list(nlp(line).sents):\n",
    "            sentences.append(str(part).strip())\n",
    "\n",
    "#           sentences += nltk.sent_tokenize(line)\n",
    "            \n",
    "    # Only interested in full sentences and sentences with 10 to 100 words.\n",
    "    sentences = [s for s in sentences if re.match('^[A-Z][^?!.]*[?.!]$', s) is not None]\n",
    "    sentences = [s.replace('\\n', ' ') for s in sentences]\n",
    "    sentences = [s for s in sentences if (len(s.split(' ')) > 10) & (len(s.split(' ')) < 100)]\n",
    "\n",
    "    return pages_content, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "report_pages, report_sentences = extract_sentences(nlp, report['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On best effort basis, excluding header and footer contents that are not relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "headers = ['Contents Our Approach to ESG Solutions for Impact How We Do Business Appendices', 'Citi 2019 ESG Report']\n",
    "\n",
    "for header in headers:\n",
    "    report_pages = [p.replace(header, \"\").strip() for p in report_pages]\n",
    "    report_sentences = [p.replace(header, \"\").strip() for p in report_sentences]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization, Bigrams and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def run_NLP(content):\n",
    "\n",
    "    def sent_to_words(sentences):\n",
    "        for sentence in sentences:\n",
    "            yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "    # Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "    def remove_stopwords(texts):\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "    def make_bigrams(texts):\n",
    "        return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "    def make_trigrams(texts):\n",
    "        return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "    def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "        texts_out = []\n",
    "        for sent in texts:\n",
    "            doc = nlp(\" \".join(sent)) \n",
    "            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        return texts_out\n",
    "\n",
    "    data_words = list(sent_to_words(content))\n",
    "\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    \n",
    "    return data_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using full text content in a page rather than full text content in a sentence here.  A page defined as a document as per TFIDF calculation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "data_lemmatized = run_NLP(report_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "report_sentences_lemma = [' '.join(w) for w in data_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'award recognition leader industry strive act responsible corporate citizen proud accomplishment honor recognize organization publication customer world selection award receive plant receive clean green advanced technology honor resource comprehensive utilization association economic information conduct survey chemical enterprise green development promote green manufacturing develop green industry plant select company honor advanced clean technology equipment carbon_black production flue gas treatment energy saving combustion technology help plant successfully achieve high production efficiency furthermore set energy center allow desulfurization denitrification tail gas steam produce deliver neighboring enterprise resource utilization offset use fossil fuel facility cabot name lead company cabot recognize lead company company annual meeting chapter integral responsibility recognition result contribution sustainable development excellent performance protection people community environment process product safety security logistic chain award recognition highlight gold level recognition cabot corporation give ecovadi outstanding pioneer safety production tianjin china_given economic technological development area teda good carbon_black supplier give cabot corporation give strategic supplier cabot corporation give linglong tire annual green operation award corporate social responsibility innovation clean green advanced technology award china_given resource comprehensive utilization association economic information commission fyp model enterprise environment protection china_given petroleum chemical industry outstanding enterprise tax contribution county credible production enterprise cabot corporation award market seminar harmonious labor relation enterprise award tianjin china_given economic technological development area teda advanced enterprise safety production management jiangxi china_given people government advance technical enterprise foreign investment advanced enterprise donate school government good enterprise social responsibility gold seal cabot brasil industria comercio give mutual assistance plan pam cabot sustainability report'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "report_sentences_lemma[random.randint(0, len(report_sentences_lemma))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant sentences are now well defined for ESG corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud - Corpus Wide Term Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# context specific keywords not to include in topic modelling\n",
    "fsi_stop_words = [\n",
    "  'plc', 'group', 'target',\n",
    "  'track', 'capital', 'holding',\n",
    "  'report', 'annualreport',\n",
    "  'esg', 'bank', 'report',\n",
    "  'annualreport', 'long', 'make'\n",
    "]\n",
    "\n",
    "fsi_stop_words.append(report_company)\n",
    "fsi_stop_words.append('citi')\n",
    "\n",
    "# our list contains all english stop words + companies names + specific keywords\n",
    "stop_words = stop_words.union(fsi_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/opt/anaconda3/envs/pyblio/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# aggregate all 7200 records into one large string to run wordcloud on term frequency\n",
    "large_string = ' '.join(report_sentences_lemma)\n",
    "\n",
    "# use 3rd party lib to compute term freq., apply stop words\n",
    "word_cloud = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    max_words=5000, \n",
    "    width=1500, \n",
    "    height=1000, \n",
    "    stopwords=stop_words, \n",
    "    contour_width=3, \n",
    "    contour_color='steelblue'\n",
    ")\n",
    "\n",
    "# display our wordcloud across all records\n",
    "plt.figure(figsize=(16,16))\n",
    "word_cloud.generate(large_string)\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF - Unigram and Bigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Run bi-gram TF-IDF frequencies\n",
    "bigram_tf_idf_vectorizer = TfidfVectorizer(stop_words=stop_words, ngram_range=(1,2), min_df=10, use_idf=True)\n",
    "bigram_tf_idf = bigram_tf_idf_vectorizer.fit_transform(report_sentences_lemma)\n",
    "\n",
    "# Extract bi-grams names\n",
    "words = bigram_tf_idf_vectorizer.get_feature_names()\n",
    "\n",
    "# extract our top 10 ngrams\n",
    "total_counts = np.zeros(len(words))\n",
    "for t in bigram_tf_idf:\n",
    "    total_counts += t.toarray()[0]\n",
    "\n",
    "count_dict = (zip(words, total_counts))\n",
    "count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:30]\n",
    "words = [w[0] for w in count_dict]\n",
    "counts = [w[1] for w in count_dict]\n",
    "x_pos = np.arange(len(words)) \n",
    "\n",
    "# Plot top 10 ngrams\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(title='30 most common uni-gram and bi-grams')\n",
    "sns.barplot(x_pos, counts, palette='Blues_r')\n",
    "plt.xticks(x_pos, words, rotation=90) \n",
    "plt.xlabel('N-grams')\n",
    "plt.ylabel('tfidf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Topic Modeling and Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Evaluaton Metrics\n",
    "1. **Perplexity**: Captures how surprised a model is of new data it has not seen before, and is measured as the normalized log-likelihood of a held-out test set.  Lower the better.\n",
    "2. **Coherence Score**: Measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic.  Higher the better.\n",
    "\n",
    "In the analysis, the model with the highest coherence score is selected as the optimal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "optimal_score = 0\n",
    "optimal_model = None\n",
    "\n",
    "for num_of_topics in tqdm(range(2,15,1)):\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                id2word=id2word,\n",
    "                                                num_topics=num_of_topics, \n",
    "                                                random_state=42,\n",
    "                                                update_every=1,\n",
    "                                                chunksize=5,\n",
    "                                                passes=20,\n",
    "                                                alpha='auto',\n",
    "                                                per_word_topics=True) \n",
    "\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    if optimal_score < coherence_model_lda.get_coherence():\n",
    "        optimal_score = coherence_model_lda.get_coherence()\n",
    "        optimal_model = lda_model\n",
    "\n",
    "    result['num_of_topics'] = num_of_topics\n",
    "    result['perplexity'] = lda_model.log_perplexity(corpus)\n",
    "    result['coherence_score'] = coherence_model_lda.get_coherence()\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "results = pd.DataFrame(results)  \n",
    "results.set_index('num_of_topics', inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot(secondary_y='perplexity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity of the Optimal Model: ', optimal_model.log_perplexity(corpus))  \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=optimal_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score of the Optimal Model: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(optimal_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the keywords in the optimal model\n",
    "pprint(optimal_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinguishing the different ESG topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=50,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = optimal_model.show_topics(formatted=False)\n",
    "\n",
    "# Currently requires manually changing the rows and columns to suit \n",
    "fig, axes = plt.subplots(2, 3, figsize=(15,15), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Notes\n",
    "1. This is only a simple analysis that studies the ESG topics in only one ESG report.  This study can be extended to more reports issued by the many companies across different sectors and industries.\n",
    "2. We can expect the number of topics to increase when more ESG reports are included in the study.\n",
    "3. Once relevant ESG topics are identified, then we can potentially calculate ESG scores by topics for all companies.  Then we can assess and see whether such ESG score derived through NLP process correlates with the score given by rating agencies downloaded from yahoo finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyblio",
   "language": "python",
   "name": "pyblio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
