{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "fwImvbgQHekw"
   },
   "source": [
    "# Analysing ESG report using Natural Language Processing\n",
    "by Chee-Foong on 12 Mar 2021, updated by Jingjie Yeo on 29 Jun 2023\n",
    "\n",
    "\n",
    "## Summary\n",
    "Environment, Social and Corporate Governance (ESG) refers to the three central factors in measuring the sustainability and societal impact of an investment in a company or business.  These criteria help to better determine the future financial performance of companies (return and risk).\n",
    "\n",
    "This analysis extracts text from a ESG report in PDF format from the internet, performs NLP on these information, summaries the key ESG initiatives with WordClouds, Tf-IDFs and discovers topics by building a Latent Dirichlet Allocation (LDA) model.\n",
    "\n",
    "As ESG is a broad topic, different companies focus on different aspects of ESG depending on their business operations and culture. We will ingest  ESG reports from different companies across several sectors and industries to capture relevant ESG topics.\n",
    "\n",
    "\n",
    "## Reference\n",
    "\n",
    "1. [A data-driven approach to Environmental, Social and Governance](https://databricks.com/blog/2020/07/10/a-data-driven-approach-to-environmental-social-and-governance.html)\n",
    "2. [Higher ESG ratings are generally positively correlated with valuation and profitability while negatively correlated with volatility.](https://corpgov.law.harvard.edu/2020/01/14/esg-matters/)\n",
    "3. [Topic Modeling with Gensim (Python)](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)\n",
    "4. [Citibank's 2019 ESG report](https://www.citigroup.com/citi/about/esg/download/2019/Global-ESG-Report-2019.pdf?ieNocache=967)\n",
    "5. [Databricks - ESG Reports](https://databricks.com/notebooks/esg_notebooks/01_esg_report.html)\n",
    "5. [Databricks - Data Driven ESG Score](https://databricks.com/notebooks/esg_notebooks/02_esg_scoring.html)\n",
    "6. [Databricks - ESG Market Risk](https://databricks.com/notebooks/esg_notebooks/03_esg_market.html)\n",
    "7. [Topic Modeling and Latent Dirichlet Allocation (LDA) in Python](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24)\n",
    "8. [Evaluate Topic Models: Latent Dirichlet Allocation (LDA)](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0)\n",
    "9. [Topic modeling visualization – How to present the results of LDA models?](https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "iYtgVkQ-Hekx"
   },
   "source": [
    "---\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k5hBFksrJHNU"
   },
   "outputs": [],
   "source": [
    "# Commands for Colab\n",
    "!git clone https://github.com/jingjieyeo/esg-nlp.git\n",
    "%cd esg-nlp\n",
    "%pip install pyldavis\n",
    "%pip install pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zFp0An7Heky"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# pd.options.display.max_columns = 50\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 50)\n",
    "\n",
    "plt.rcParams.update({'figure.figsize':(15,6), 'figure.dpi':60})\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HR9MG4fbHeky"
   },
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.append('/content/esg-nlp/src') \n",
    "from edge import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSmhSacRHekz",
    "outputId": "fd294d79-ff01-4304-c325-fc5b39cb9478"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: pyldavis in /usr/local/lib/python3.7/dist-packages (3.3.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyldavis) (1.4.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyldavis) (1.1.0)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyldavis) (2.8.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyldavis) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyldavis) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyldavis) (57.4.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyldavis) (1.21.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyldavis) (2.11.3)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyldavis) (0.0)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyldavis) (3.6.0)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyldavis) (1.3.5)\n",
      "Requirement already satisfied: funcy in /usr/local/lib/python3.7/dist-packages (from pyldavis) (1.17)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyldavis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyldavis) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyldavis) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyldavis) (6.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyldavis) (2.0.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from numexpr->pyldavis) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->numexpr->pyldavis) (3.0.9)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyldavis) (3.1.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pdfminer\n",
      "  Downloading pdfminer-20191125.tar.gz (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 6.9 MB/s \n",
      "\u001b[?25hCollecting pycryptodome\n",
      "  Downloading pycryptodome-3.14.1-cp35-abi3-manylinux2010_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 54.0 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pdfminer\n",
      "  Building wheel for pdfminer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140087 sha256=01213d39d220884c9f5f125932a3dfad2045fa6956b3b3548b526299f66b4fb5\n",
      "  Stored in directory: /root/.cache/pip/wheels/e3/5e/f4/d210b46e9e4a28229ea070ed5b3efa92c3c29d1a7918dd4b97\n",
      "Successfully built pdfminer\n",
      "Installing collected packages: pycryptodome, pdfminer\n",
      "Successfully installed pdfminer-20191125 pycryptodome-3.14.1\n"
     ]
    }
   ],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "\n",
    "# PDF text extraction\n",
    "from pdfminer.layout import LAParams, LTTextBox\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.converter import TextConverter\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "# Others\n",
    "import requests\n",
    "import io\n",
    "import string\n",
    "import re\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "import io\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AI0QB1ZoHekz"
   },
   "outputs": [],
   "source": [
    "# Create Folder to store .json data\n",
    "DATA_FOLDER = '/content/esg-nlp/data/'\n",
    "createfolder(DATA_FOLDER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B0KG903hHek0"
   },
   "source": [
    "### Loading Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1SwIGRgHek0"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XDwm76viHek1"
   },
   "source": [
    "### Preparing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duGkVGIoHek1"
   },
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmsCYR4XHek1"
   },
   "outputs": [],
   "source": [
    "', '.join(stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KHNtoaOJHek2"
   },
   "source": [
    "## Report Details\n",
    "\n",
    "Enter required information about the report.  Report must be in PDF format and downloadable from a URL.\n",
    "\n",
    "1. Company Name\n",
    "2. Company Ticker Symbol\n",
    "3. Year of the Report\n",
    "4. URL of the Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUwCSs4QHek2"
   },
   "outputs": [],
   "source": [
    "# Cabot Corp\n",
    "report_company = 'cabot'\n",
    "report_ticker = 'CBT'\n",
    "report_year = '2023'\n",
    "report_url = 'https://www.cabotcorp.com/-/media/files/reports/responsibility/cabot-corporation-sustainability-report-2023.pdf?la=en&rev=8836b8677f4040e992ab5d52c9b86412'\n",
    "\n",
    "#report_company = 'cabot'\n",
    "#report_ticker = 'CBT'\n",
    "#report_year = '2021'\n",
    "#report_url = 'https://www.cabotcorp.com/-/media/files/reports/responsibility/#cabot-corporation-sustainability-report-2021.pdf?la=en&rev=5b2b1b0ee3154b5a9a8f32e772b10a2d'\n",
    "\n",
    "# MIT\n",
    "#report_company = 'mit'\n",
    "#report_ticker = 'nothing'\n",
    "#report_year = '2021'\n",
    "#report_url = 'https://sustainability.mit.edu/sites/default/files/2022-01/presidents_report_2021_web.pdf'\n",
    "\n",
    "# General Motors\n",
    "#report_company = 'gm'\n",
    "#report_ticker = 'GM'\n",
    "#report_year = '2021'\n",
    "#report_url = 'https://www.gmsustainability.com/_pdf/resources-and-downloads/GM_2021_SR.pdf'\n",
    "\n",
    "# Microsoft\n",
    "#report_company = 'microsoft'\n",
    "#report_ticker = 'MSFT'\n",
    "#report_year = '2021'\n",
    "#report_url = 'https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4RwfV'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Z384H_JsHek3"
   },
   "source": [
    "## Text Extraction\n",
    "Extract information from the PDF report.  This process may take some time.  Do be patient.\n",
    "\n",
    "*You may skip to the last step of this section if you have previously extracted the contents and stored in a json file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8as4uG1Hek3"
   },
   "outputs": [],
   "source": [
    "def extract_pdf(file, verbose=False):\n",
    "    \n",
    "    if verbose:\n",
    "        print('Processing {}'.format(file))\n",
    "\n",
    "    try:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        laparams = LAParams()\n",
    "\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, laparams=laparams)\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        password = \"\"\n",
    "        maxpages = 0\n",
    "        pagenos = set()\n",
    "\n",
    "        content = []\n",
    "\n",
    "        for page in PDFPage.get_pages(file,\n",
    "                                      pagenos, \n",
    "                                      maxpages=maxpages,\n",
    "                                      password=password,\n",
    "                                      caching=True,\n",
    "                                      check_extractable=False):\n",
    "\n",
    "            page_interpreter.process_page(page)\n",
    "\n",
    "            content.append(fake_file_handle.getvalue())\n",
    "\n",
    "            fake_file_handle.truncate(0)\n",
    "            fake_file_handle.seek(0)        \n",
    "\n",
    "        text = '##PAGE_BREAK##'.join(content)\n",
    "\n",
    "        # close open handles\n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # close open handles\n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIzltfj2Hek3"
   },
   "outputs": [],
   "source": [
    "def extract_content(url):\n",
    "    \"\"\"\n",
    "    A simple user define function that, given a url, download PDF text content\n",
    "    Parse PDF and return plain text version\n",
    "    \"\"\"\n",
    "    headers={\"User-Agent\":\"Mozilla/5.0\"}\n",
    "\n",
    "    try:\n",
    "        # retrieve PDF binary stream\n",
    "        r = requests.get(url, allow_redirects=True, headers=headers)\n",
    "        \n",
    "        # access pdf content\n",
    "        text = extract_pdf(io.BytesIO(r.content))\n",
    "\n",
    "        # return concatenated content\n",
    "        return text\n",
    "\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08ALKuukHek3"
   },
   "outputs": [],
   "source": [
    "report_content = extract_content(report_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d42K7laiHek4"
   },
   "outputs": [],
   "source": [
    "#Storing all information in a json file\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mufoyLZ_Hek4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report = {'company':report_company, 'year':report_year, 'ticker':report_ticker, \n",
    "          'url':report_url, 'content':report_content}\n",
    "\n",
    "with open(DATA_FOLDER + report_company + report_year + '.json', \"w\") as outfile:  \n",
    "    json.dump(report, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrGTdHmOHek4"
   },
   "outputs": [],
   "source": [
    "# Reloading the json file when required\n",
    "with open(DATA_FOLDER + report_company + report_year + '.json') as inputfile:\n",
    "     report = json.load(inputfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1wnTxiGEHek4"
   },
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "### Extracting content by pages and sentences\n",
    "\n",
    "1. Only properly structured sentences are extracted.  Sentences that start with a Capital Letter and ends with a period.  Sentences that are less than 10 words or more than 50 words are ignored.  \n",
    "2. Pages with less than 500 words are excluded from extraction.  Would like to focus on pages with mostly text content.  This should exclude pages like cover and last page.  It should also exclude pages that are more graphical with short notes and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKy4yEiCHek4"
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    printable = set(string.printable)\n",
    "    return ''.join(filter(lambda x: x in printable, text))\n",
    "\n",
    "def not_header(line):\n",
    "    # as we're consolidating broken lines into paragraphs, we want to make sure not to include headers\n",
    "    return not line.isupper()\n",
    "\n",
    "def extract_sentences(nlp, text):\n",
    "    \"\"\"\n",
    "    Extracting ESG statements from raw text by removing junk, URLs, etc.\n",
    "    We group consecutive lines into paragraphs and use spacy to parse sentences.\n",
    "    \"\"\"\n",
    "    MIN_WORDS_PER_PAGE = 500\n",
    "    \n",
    "    pages = text.split('##PAGE_BREAK##')\n",
    "#     print('Number of Pages: {}'.format(len(pages)))\n",
    "\n",
    "    lines = []\n",
    "    for page in pages:\n",
    "        \n",
    "        # remove non ASCII characters\n",
    "        text = remove_non_ascii(page)\n",
    "        \n",
    "        if len(text.split(' ')) < MIN_WORDS_PER_PAGE:\n",
    "#             print('Skipped Page: {}'.format(len(text.split(' '))))\n",
    "            continue\n",
    "        \n",
    "        prev = \"\"\n",
    "        for line in text.split('\\n\\n'):\n",
    "            # aggregate consecutive lines where text may be broken down\n",
    "            # only if next line starts with a space or previous does not end with dot.\n",
    "            if(line.startswith(' ') or not prev.endswith('.')):\n",
    "                prev = prev + ' ' + line\n",
    "            else:\n",
    "                # new paragraph\n",
    "                lines.append(prev)\n",
    "                prev = line\n",
    "\n",
    "        # don't forget left-over paragraph\n",
    "        lines.append(prev)\n",
    "        lines.append('##SAME_PAGE##')\n",
    "        \n",
    "    lines = '  '.join(lines).split('##SAME_PAGE##')\n",
    "    \n",
    "    # clean paragraphs from extra space, unwanted characters, urls, etc.\n",
    "    # best effort clean up, consider a more versatile cleaner\n",
    "    \n",
    "    sentences = []\n",
    "    pages_content = []\n",
    "\n",
    "    for line in lines[:-1]:\n",
    "        # removing header number\n",
    "        line = re.sub(r'^\\s?\\d+(.*)$', r'\\1', line)\n",
    "        # removing trailing spaces\n",
    "        line = line.strip()\n",
    "        # words may be split between lines, ensure we link them back together\n",
    "        line = re.sub(r'\\s?-\\s?', '-', line)\n",
    "        # remove space prior to punctuation\n",
    "        line = re.sub(r'\\s?([,:;\\.])', r'\\1', line)\n",
    "        # ESG contains a lot of figures that are not relevant to grammatical structure\n",
    "        line = re.sub(r'\\d{5,}', r' ', line)\n",
    "        # remove emails\n",
    "        line = re.sub(r'\\S*@\\S*\\s?', '', line)\n",
    "        # remove mentions of URLs\n",
    "        line = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', r' ', line)\n",
    "        # remove multiple spaces\n",
    "        line = re.sub(r'\\s+', ' ', line)\n",
    "        # join next line with space\n",
    "        line = re.sub(r' \\n', ' ', line)\n",
    "        line = re.sub(r'.\\n', '. ', line)\n",
    "        line = re.sub(r'\\x0c', ' ', line)\n",
    "        \n",
    "        pages_content.append(str(line).strip())\n",
    "\n",
    "        # split paragraphs into well defined sentences using spacy\n",
    "        for part in list(nlp(line).sents):\n",
    "            sentences.append(str(part).strip())\n",
    "\n",
    "#           sentences += nltk.sent_tokenize(line)\n",
    "            \n",
    "    # Only interested in full sentences and sentences with 10 to 100 words.\n",
    "    sentences = [s for s in sentences if re.match('^[A-Z][^?!.]*[?.!]$', s) is not None]\n",
    "    sentences = [s.replace('\\n', ' ') for s in sentences]\n",
    "    sentences = [s for s in sentences if (len(s.split(' ')) > 10) & (len(s.split(' ')) < 100)]\n",
    "\n",
    "    return pages_content, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IcfOeEAUHek4"
   },
   "outputs": [],
   "source": [
    "report_pages, report_sentences = extract_sentences(nlp, report['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFFqnY5AHek4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On best effort basis, excluding header and footer contents that are not relevant.\n",
    "# May or may not be needed depending on format of reports.\n",
    "# Ignore for now\n",
    "\"\"\"\n",
    "headers = ['CABOT CORPORATION 2016 SUSTAINABILITY REPORT', 'CABOT CORPORATION 2020 SUSTAINABILITY REPORT']\n",
    "\n",
    "for header in headers:\n",
    "    report_pages = [p.replace(header, \"\").strip() for p in report_pages]\n",
    "    report_sentences = [p.replace(header, \"\").strip() for p in report_sentences]   \n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aq8dMD7ZHek4"
   },
   "source": [
    "### Tokenization, Bigrams and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZIlM8EiHek4"
   },
   "outputs": [],
   "source": [
    "def run_NLP(content):\n",
    "\n",
    "    def sent_to_words(sentences):\n",
    "        for sentence in sentences:\n",
    "            yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "    # Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "    def remove_stopwords(texts):\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "    def make_bigrams(texts):\n",
    "        return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "    def make_trigrams(texts):\n",
    "        return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "    def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "        texts_out = []\n",
    "        for sent in texts:\n",
    "            doc = nlp(\" \".join(sent)) \n",
    "            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        return texts_out\n",
    "\n",
    "    data_words = list(sent_to_words(content))\n",
    "\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    \n",
    "    return data_lemmatized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3FUTF7vLHek5"
   },
   "source": [
    "#### Using full text content in a page rather than full text content in a sentence here.  A page defined as a document as per TFIDF calculation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCgNIew7Hek5"
   },
   "outputs": [],
   "source": [
    "data_lemmatized = run_NLP(report_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCikE4gGHek5"
   },
   "outputs": [],
   "source": [
    "report_sentences_lemma = [' '.join(w) for w in data_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EU8kzvd2Hek5"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "report_sentences_lemma[random.randint(0, len(report_sentences_lemma))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sP9KCNdrHek5"
   },
   "source": [
    "#### Relevant sentences are now well defined for ESG corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uh_FT4PWHek5"
   },
   "source": [
    "### Word Cloud - Corpus Wide Term Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_1AUIENHek5"
   },
   "outputs": [],
   "source": [
    "# Context specific keywords not to include in topic modelling\n",
    "# Need to tune accordingly but we will leave it as it is for now\n",
    "\n",
    "fsi_stop_words = [\n",
    "  'plc', 'group', 'target',\n",
    "  'track', 'capital', 'holding',\n",
    "  'report', 'annualreport',\n",
    "  'esg', 'bank', 'report',\n",
    "  'annualreport', 'long', 'make'\n",
    "]\n",
    "\n",
    "fsi_stop_words.append(report_company)\n",
    "\n",
    "# our list contains all english stop words + companies names + specific keywords\n",
    "stop_words = stop_words.union(fsi_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rgEALxnHek6"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# aggregate all 7200 records into one large string to run wordcloud on term frequency\n",
    "large_string = ' '.join(report_sentences_lemma)\n",
    "\n",
    "# use 3rd party lib to compute term freq., apply stop words\n",
    "word_cloud = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    max_words=5000, \n",
    "    width=1500, \n",
    "    height=1000, \n",
    "    stopwords=stop_words, \n",
    "    contour_width=3, \n",
    "    contour_color='steelblue'\n",
    ")\n",
    "\n",
    "# display our wordcloud across all records\n",
    "plt.figure(figsize=(16,16))\n",
    "word_cloud.generate(large_string)\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5aRmubwDHek6"
   },
   "source": [
    "### TFIDF - Unigram and Bigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gb9ETgPoHek6"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Run bi-gram TF-IDF frequencies\n",
    "bigram_tf_idf_vectorizer = TfidfVectorizer(stop_words=list(stop_words), ngram_range=(1,2), min_df=1, use_idf=True)\n",
    "bigram_tf_idf = bigram_tf_idf_vectorizer.fit_transform(report_sentences_lemma)\n",
    "\n",
    "# Extract bi-grams names\n",
    "words = bigram_tf_idf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# extract our top 10 ngrams\n",
    "total_counts = np.zeros(len(words))\n",
    "for t in bigram_tf_idf:\n",
    "    total_counts += t.toarray()[0]\n",
    "\n",
    "count_dict = (zip(words, total_counts))\n",
    "count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:30]\n",
    "words = [w[0] for w in count_dict]\n",
    "counts = [w[1] for w in count_dict]\n",
    "x_pos = np.arange(len(words)) \n",
    "\n",
    "# Plot top 10 ngrams\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(title='30 most common uni-gram and bi-grams')\n",
    "sns.barplot(x=x_pos, y=counts, palette='Blues_r')\n",
    "plt.xticks(x_pos, words, rotation=90) \n",
    "plt.xlabel('N-grams')\n",
    "plt.ylabel('tfidf')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4Pvb7r3bHek6"
   },
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TREDmKKVHek6"
   },
   "source": [
    "\n",
    "## Topic Modeling and Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Evaluaton Metrics\n",
    "1. **Perplexity**: Captures how surprised a model is of new data it has not seen before, and is measured as the normalized log-likelihood of a held-out test set.  Lower the better.\n",
    "2. **Coherence Score**: Measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic.  Higher the better.\n",
    "\n",
    "In the analysis, the model with the highest coherence score is selected as the optimal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_gqlMQ1Hek6"
   },
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G68aRIPcHek7"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "optimal_score = 0\n",
    "optimal_model = None\n",
    "\n",
    "for num_of_topics in tqdm(range(2,15,1)):\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                id2word=id2word,\n",
    "                                                num_topics=num_of_topics, \n",
    "                                                random_state=42,\n",
    "                                                update_every=1,\n",
    "                                                chunksize=5,\n",
    "                                                passes=20,\n",
    "                                                alpha='auto',\n",
    "                                                per_word_topics=True) \n",
    "\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    if optimal_score < coherence_model_lda.get_coherence():\n",
    "        optimal_score = coherence_model_lda.get_coherence()\n",
    "        optimal_model = lda_model\n",
    "\n",
    "    result['num_of_topics'] = num_of_topics\n",
    "    result['perplexity'] = lda_model.log_perplexity(corpus)\n",
    "    result['coherence_score'] = coherence_model_lda.get_coherence()\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "results = pd.DataFrame(results)  \n",
    "results.set_index('num_of_topics', inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3QDjg2p3Hek8"
   },
   "outputs": [],
   "source": [
    "results.plot(secondary_y='perplexity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIDNo-rhHek8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywe9lvXHHek8"
   },
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity of the Optimal Model: ', optimal_model.log_perplexity(corpus))  \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=optimal_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score of the Optimal Model: ', coherence_lda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3XtZBb-VHek8"
   },
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "i7NjJwR_Hek8"
   },
   "source": [
    "## Visualize the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iv0XMi2FHek8"
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(optimal_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPl-piIYHek8"
   },
   "outputs": [],
   "source": [
    "# Print the keywords in the optimal model\n",
    "pprint(optimal_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zqezjjCoHek8"
   },
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uHjH76lfHek8"
   },
   "source": [
    "## Distinguishing the different ESG topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCT7OVjKHek8"
   },
   "outputs": [],
   "source": [
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=50,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = optimal_model.show_topics(formatted=False)\n",
    "\n",
    "# Currently requires manually changing the rows and columns to suit \n",
    "fig, axes = plt.subplots(4, 3, figsize=(15,15), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jAn-mqZvHek8"
   },
   "source": [
    "# Closing Notes\n",
    "1. This is only a simple analysis that studies the ESG topics in only one ESG report.  This study can be extended to more reports issued by the many companies across different sectors and industries.\n",
    "2. We can expect the number of topics to increase when more ESG reports are included in the study.\n",
    "3. Once relevant ESG topics are identified, then we can potentially calculate ESG scores by topics for all companies.  Then we can assess and see whether such ESG score derived through NLP process correlates with the score given by rating agencies downloaded from Yahoo Finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlxCBUe8Hek8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "aq8dMD7ZHek4",
    "3FUTF7vLHek5",
    "sP9KCNdrHek5",
    "uh_FT4PWHek5",
    "5aRmubwDHek6"
   ],
   "name": "Copy of cabot-sustainability-report-analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pyblio",
   "language": "python",
   "name": "pyblio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
